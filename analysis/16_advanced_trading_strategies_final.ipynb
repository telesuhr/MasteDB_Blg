{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Trading Strategies Analysis - 高度な取引戦略分析\n",
    "\n",
    "このノートブックでは、LME銅先物データを使用して、より高度な取引戦略と市場分析を実施します。\n",
    "\n",
    "## Analysis Overview - 分析の概要\n",
    "1. **Spread Trading Strategy** - スプレッド取引戦略分析\n",
    "2. **Seasonality Analysis** - 季節性・循環性分析\n",
    "3. **Correlation Structure** - 相関構造分析\n",
    "4. **Risk Parity Analysis** - リスク・パリティ分析\n",
    "5. **Anomaly Detection** - 異常検知・レジーム分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.database_config import get_connection_string\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# プロジェクトルートをPythonパスに追加\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 英語フォント設定\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'primary': '#1f77b4',\n",
    "    'secondary': '#ff7f0e',\n",
    "    'tertiary': '#2ca02c',\n",
    "    'quaternary': '#d62728',\n",
    "    'quinary': '#9467bd'\n",
    "}\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading - データ読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_futures_data(conn, days=365):\n",
    "    \"\"\"Load copper futures data\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        p.TradeDate,\n",
    "        t.TenorTypeName,\n",
    "        p.SettlementPrice as ClosePrice,\n",
    "        p.Volume,\n",
    "        p.OpenInterest,\n",
    "        CASE \n",
    "            WHEN t.TenorTypeName LIKE 'Generic 1st%' THEN 1\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 2nd%' THEN 2\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 3rd%' THEN 3\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 4th%' THEN 4\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 5th%' THEN 5\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 6th%' THEN 6\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 7th%' THEN 7\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 8th%' THEN 8\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 9th%' THEN 9\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 10th%' THEN 10\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 11th%' THEN 11\n",
    "            WHEN t.TenorTypeName LIKE 'Generic 12th%' THEN 12\n",
    "            ELSE NULL\n",
    "        END as TenorNumber\n",
    "    FROM T_CommodityPrice p\n",
    "    INNER JOIN M_Metal m ON p.MetalID = m.MetalID\n",
    "    INNER JOIN M_TenorType t ON p.TenorTypeID = t.TenorTypeID\n",
    "    WHERE \n",
    "        m.MetalCode = 'COPPER'\n",
    "        AND p.TradeDate >= DATEADD(day, -{days}, GETDATE())\n",
    "        AND p.SettlementPrice IS NOT NULL\n",
    "    ORDER BY p.TradeDate DESC\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df['TradeDate'] = pd.to_datetime(df['TradeDate'])\n",
    "    df = df.dropna(subset=['TenorNumber'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Database connection and data loading\n",
    "conn = pyodbc.connect(get_connection_string())\n",
    "df = load_futures_data(conn, days=365)\n",
    "\n",
    "# Create pivot tables\n",
    "price_pivot = df.pivot_table(values='ClosePrice', index='TradeDate',\n",
    "                             columns='TenorNumber', aggfunc='mean').sort_index()\n",
    "volume_pivot = df.pivot_table(\n",
    "    values='Volume', index='TradeDate', columns='TenorNumber', aggfunc='mean').sort_index()\n",
    "\n",
    "# Get available tenors\n",
    "available_tenors = sorted(\n",
    "    [col for col in price_pivot.columns if not pd.isna(col)])\n",
    "\n",
    "print(\n",
    "    f\"Data period: {df['TradeDate'].min().date()} - {df['TradeDate'].max().date()}\")\n",
    "print(f\"Data records: {len(df):,}\")\n",
    "print(f\"Available tenors: {available_tenors}\")\n",
    "print(f\"Price data shape: {price_pivot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spread Trading Strategy Analysis\n",
    "\n",
    "### Analysis Objective - 分析目的\n",
    "- Identify arbitrage opportunities using price differences between different contract months\n",
    "- Evaluate risk and return of calendar spread strategies\n",
    "\n",
    "### Methodology - 分析手法\n",
    "- Calendar spread calculation (near vs far months)\n",
    "- Statistical properties analysis\n",
    "- Mean reversion verification\n",
    "- Risk-adjusted return calculation\n",
    "\n",
    "### Interpretation - グラフの見方・解釈\n",
    "- Positive spread: Contango (far month > near month)\n",
    "- Negative spread: Backwardation (near month > far month)\n",
    "- Spread volatility: Trading opportunity variation\n",
    "- Mean reversion coefficient: Spread stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread analysis\n",
    "if len(available_tenors) >= 2:\n",
    "    near_month = available_tenors[0]\n",
    "    far_month = available_tenors[1] if len(\n",
    "        available_tenors) > 1 else available_tenors[0]\n",
    "\n",
    "    # Calculate spread (far month - near month)\n",
    "    spread = price_pivot[far_month] - price_pivot[near_month]\n",
    "    spread = spread.dropna()\n",
    "\n",
    "    # Spread statistics\n",
    "    spread_stats = {\n",
    "        'Mean': spread.mean(),\n",
    "        'Std Dev': spread.std(),\n",
    "        'Max': spread.max(),\n",
    "        'Min': spread.min(),\n",
    "        'Sharpe Ratio': spread.mean() / spread.std() if spread.std() != 0 else 0\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSpread Statistics (M{far_month} - M{near_month}):\")\n",
    "    for key, value in spread_stats.items():\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Spread time series\n",
    "    axes[0, 0].plot(spread.index, spread.values, linewidth=1, color='blue')\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_title(f'Calendar Spread (M{far_month} - M{near_month})')\n",
    "    axes[0, 0].set_ylabel('Spread (USD)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Spread distribution\n",
    "    axes[0, 1].hist(spread.values, bins=30, alpha=0.7,\n",
    "                    color='green', edgecolor='black')\n",
    "    axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_title('Spread Distribution')\n",
    "    axes[0, 1].set_xlabel('Spread (USD)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "    # 3. Price time series comparison\n",
    "    axes[1, 0].plot(price_pivot.index, price_pivot[near_month],\n",
    "                    label=f'M{near_month}', linewidth=1)\n",
    "    axes[1, 0].plot(price_pivot.index, price_pivot[far_month],\n",
    "                    label=f'M{far_month}', linewidth=1)\n",
    "    axes[1, 0].set_title('Price Evolution by Tenor')\n",
    "    axes[1, 0].set_ylabel('Price (USD)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Correlation scatter plot\n",
    "    common_dates = price_pivot.dropna(subset=[near_month, far_month])\n",
    "    if len(common_dates) > 0:\n",
    "        axes[1, 1].scatter(common_dates[near_month],\n",
    "                           common_dates[far_month], alpha=0.6, s=20)\n",
    "        axes[1, 1].set_xlabel(f'M{near_month} Price (USD)')\n",
    "        axes[1, 1].set_ylabel(f'M{far_month} Price (USD)')\n",
    "        axes[1, 1].set_title('Inter-Tenor Correlation')\n",
    "\n",
    "        # Calculate correlation\n",
    "        correlation = common_dates[near_month].corr(common_dates[far_month])\n",
    "        axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
    "                        transform=axes[1, 1].transAxes,\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Spread analysis requires at least 2 tenors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seasonality Analysis\n",
    "\n",
    "### Analysis Objective - 分析目的\n",
    "- Identify seasonal patterns and cyclical behavior in copper prices\n",
    "- Capture predictable price movements\n",
    "\n",
    "### Methodology - 分析手法\n",
    "- Monthly and quarterly return analysis\n",
    "- Day-of-week effect verification\n",
    "- Moving average deviation analysis\n",
    "\n",
    "### Interpretation - グラフの見方・解釈\n",
    "- Monthly returns: Seasonality exists if specific months show bias\n",
    "- Day-of-week effect: Structural market factors if abnormal returns on specific days\n",
    "- Cyclical patterns: Regular price movement patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality analysis\n",
    "if len(available_tenors) > 0:\n",
    "    # Use primary tenor\n",
    "    main_tenor = available_tenors[0]\n",
    "    price_series = price_pivot[main_tenor].dropna()\n",
    "\n",
    "    # Calculate daily returns\n",
    "    returns = price_series.pct_change().dropna()\n",
    "\n",
    "    # Add month, weekday, quarter information\n",
    "    returns_df = pd.DataFrame({\n",
    "        'returns': returns,\n",
    "        'month': returns.index.month,\n",
    "        'weekday': returns.index.weekday,\n",
    "        'quarter': returns.index.quarter,\n",
    "        'year': returns.index.year\n",
    "    })\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Monthly average returns\n",
    "    monthly_returns = returns_df.groupby('month')['returns'].mean()\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "    bars1 = axes[0, 0].bar(range(1, 13), monthly_returns.values,\n",
    "                           color=[\n",
    "                               'red' if x < 0 else 'green' for x in monthly_returns.values],\n",
    "                           alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Monthly Average Returns')\n",
    "    axes[0, 0].set_xlabel('Month')\n",
    "    axes[0, 0].set_ylabel('Average Return')\n",
    "    axes[0, 0].set_xticks(range(1, 13))\n",
    "    axes[0, 0].set_xticklabels(month_names, rotation=45)\n",
    "    axes[0, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Weekday average returns\n",
    "    weekday_returns = returns_df.groupby('weekday')['returns'].mean()\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "    bars2 = axes[0, 1].bar(range(7), weekday_returns.values,\n",
    "                           color=[\n",
    "                               'red' if x < 0 else 'green' for x in weekday_returns.values],\n",
    "                           alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('Weekday Average Returns')\n",
    "    axes[0, 1].set_xlabel('Day of Week')\n",
    "    axes[0, 1].set_ylabel('Average Return')\n",
    "    axes[0, 1].set_xticks(range(7))\n",
    "    axes[0, 1].set_xticklabels(weekday_names, rotation=45)\n",
    "    axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Quarterly return distribution\n",
    "    quarter_data = [returns_df[returns_df['quarter'] == q]\n",
    "                    ['returns'].values for q in range(1, 5)]\n",
    "    bp = axes[1, 0].boxplot(quarter_data, labels=[\n",
    "                            'Q1', 'Q2', 'Q3', 'Q4'], patch_artist=True)\n",
    "\n",
    "    # Color the boxplots\n",
    "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    axes[1, 0].set_title('Quarterly Return Distribution')\n",
    "    axes[1, 0].set_xlabel('Quarter')\n",
    "    axes[1, 0].set_ylabel('Return')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Cumulative returns seasonality\n",
    "    yearly_cumulative = returns_df.groupby(['year', 'month'])[\n",
    "        'returns'].sum().reset_index()\n",
    "    yearly_pivot = yearly_cumulative.pivot(\n",
    "        index='month', columns='year', values='returns')\n",
    "\n",
    "    for year in yearly_pivot.columns:\n",
    "        if not yearly_pivot[year].isna().all():\n",
    "            axes[1, 1].plot(yearly_pivot.index, yearly_pivot[year].cumsum(),\n",
    "                            label=str(year), alpha=0.7, linewidth=1)\n",
    "\n",
    "    axes[1, 1].set_title('Annual Monthly Cumulative Returns')\n",
    "    axes[1, 1].set_xlabel('Month')\n",
    "    axes[1, 1].set_ylabel('Cumulative Return')\n",
    "    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical test results\n",
    "    print(\"\\nSeasonality Analysis Results:\")\n",
    "    print(\n",
    "        f\"Best monthly return: {month_names[monthly_returns.idxmax()-1]} ({monthly_returns.max():.4f})\")\n",
    "    print(\n",
    "        f\"Worst monthly return: {month_names[monthly_returns.idxmin()-1]} ({monthly_returns.min():.4f})\")\n",
    "    print(\n",
    "        f\"Best weekday return: {weekday_names[weekday_returns.idxmax()]} ({weekday_returns.max():.4f})\")\n",
    "    print(\n",
    "        f\"Worst weekday return: {weekday_names[weekday_returns.idxmin()]} ({weekday_returns.min():.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for seasonality analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Structure Analysis\n",
    "\n",
    "### Analysis Objective - 分析目的\n",
    "- Analyze correlation relationships between different contract months\n",
    "- Evaluate portfolio risk management and diversification effects\n",
    "\n",
    "### Methodology - 分析手法\n",
    "- Inter-tenor correlation matrix calculation\n",
    "- Dynamic correlation analysis (rolling correlation)\n",
    "- Time series changes in correlations\n",
    "\n",
    "### Interpretation - グラフの見方・解釈\n",
    "- High correlation (>0.8): Strong price linkage between tenors\n",
    "- Low correlation (<0.5): Diversification benefits expected\n",
    "- Correlation changes: Watch for correlation increases during market stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(available_tenors) >= 2:\n",
    "    # Calculate daily returns for each tenor\n",
    "    returns_matrix = price_pivot.pct_change().dropna()\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = returns_matrix.corr()\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Correlation matrix heatmap\n",
    "    im1 = axes[0, 0].imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0, 0].set_title('Inter-Tenor Correlation Matrix')\n",
    "    axes[0, 0].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axes[0, 0].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axes[0, 0].set_xticklabels(\n",
    "        [f'M{int(col)}' for col in correlation_matrix.columns], rotation=45)\n",
    "    axes[0, 0].set_yticklabels(\n",
    "        [f'M{int(col)}' for col in correlation_matrix.columns])\n",
    "\n",
    "    # Add correlation coefficients as text\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(len(correlation_matrix.columns)):\n",
    "            text = axes[0, 0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                                   ha='center', va='center',\n",
    "                                   color='white' if abs(correlation_matrix.iloc[i, j]) > 0.5 else 'black')\n",
    "\n",
    "    plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)\n",
    "\n",
    "    # 2. Rolling correlation (first two tenors)\n",
    "    if len(available_tenors) >= 2:\n",
    "        tenor1, tenor2 = available_tenors[0], available_tenors[1]\n",
    "        rolling_corr = returns_matrix[tenor1].rolling(\n",
    "            window=30).corr(returns_matrix[tenor2]).dropna()\n",
    "\n",
    "        axes[0, 1].plot(rolling_corr.index, rolling_corr.values,\n",
    "                        linewidth=1, color='blue')\n",
    "        axes[0, 1].set_title(f'Rolling Correlation (M{tenor1} vs M{tenor2})')\n",
    "        axes[0, 1].set_ylabel('30-day Rolling Correlation')\n",
    "        axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0, 1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "        axes[0, 1].axhline(y=0.8, color='green', linestyle='--', alpha=0.5)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Principal Component Analysis (if sklearn available)\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        # Data standardization\n",
    "        scaler = StandardScaler()\n",
    "        returns_scaled = scaler.fit_transform(returns_matrix)\n",
    "\n",
    "        # PCA execution\n",
    "        pca = PCA()\n",
    "        pca.fit(returns_scaled)\n",
    "\n",
    "        # Visualization of explained variance\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        axes[1, 0].bar(range(1, len(explained_variance) + 1), explained_variance,\n",
    "                       alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 0].plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
    "                        color='red', marker='o', linewidth=2, markersize=4)\n",
    "        axes[1, 0].set_title(\n",
    "            'Principal Component Analysis: Explained Variance')\n",
    "        axes[1, 0].set_xlabel('Principal Component')\n",
    "        axes[1, 0].set_ylabel('Explained Variance Ratio')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        pca_available = True\n",
    "\n",
    "    except ImportError:\n",
    "        axes[1, 0].text(0.5, 0.5, 'sklearn not available\\nfor PCA analysis',\n",
    "                        ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Principal Component Analysis: Not Available')\n",
    "        pca_available = False\n",
    "\n",
    "    # 4. Correlation distribution\n",
    "    # Get upper triangle correlation coefficients\n",
    "    upper_triangle = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    correlations = upper_triangle.stack().values\n",
    "\n",
    "    axes[1, 1].hist(correlations, bins=20, alpha=0.7,\n",
    "                    color='lightgreen', edgecolor='black')\n",
    "    axes[1, 1].axvline(x=np.mean(correlations), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(correlations):.3f}')\n",
    "    axes[1, 1].set_title('Inter-Tenor Correlation Distribution')\n",
    "    axes[1, 1].set_xlabel('Correlation Coefficient')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical summary\n",
    "    print(\"\\nCorrelation Analysis Results:\")\n",
    "    print(f\"Average correlation: {np.mean(correlations):.3f}\")\n",
    "    print(f\"Maximum correlation: {np.max(correlations):.3f}\")\n",
    "    print(f\"Minimum correlation: {np.min(correlations):.3f}\")\n",
    "\n",
    "    if pca_available:\n",
    "        print(f\"1st PC explained variance: {explained_variance[0]:.3f}\")\n",
    "        if len(explained_variance) > 1:\n",
    "            print(f\"2nd PC explained variance: {explained_variance[1]:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Correlation analysis requires at least 2 tenors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Risk Parity Analysis\n",
    "\n",
    "### Analysis Objective - 分析目的\n",
    "- Construct portfolios that equalize risk contribution from each tenor\n",
    "- Optimize risk-adjusted performance\n",
    "\n",
    "### Methodology - 分析手法\n",
    "- Calculate volatility for each tenor\n",
    "- Calculate risk parity weights\n",
    "- Backtest optimized portfolios\n",
    "- Compare Sharpe ratios and risk metrics\n",
    "\n",
    "### Interpretation - グラフの見方・解釈\n",
    "- Equal weight vs Risk parity: Effect of risk adjustment\n",
    "- Cumulative returns: Long-term performance comparison\n",
    "- Drawdown: Maximum loss periods and magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk parity analysis\n",
    "if len(available_tenors) >= 2:\n",
    "    # Calculate returns and volatility for each tenor\n",
    "    returns_clean = returns_matrix.dropna()\n",
    "\n",
    "    # Annualized volatility for each tenor\n",
    "    volatilities = returns_clean.std() * np.sqrt(252)\n",
    "\n",
    "    # Risk parity weights (inverse volatility)\n",
    "    inverse_vol = 1 / volatilities\n",
    "    risk_parity_weights = inverse_vol / inverse_vol.sum()\n",
    "\n",
    "    # Equal weights\n",
    "    equal_weights = np.ones(len(available_tenors)) / len(available_tenors)\n",
    "\n",
    "    # Portfolio returns calculation\n",
    "    equal_weighted_returns = (returns_clean * equal_weights).sum(axis=1)\n",
    "    risk_parity_returns = (returns_clean * risk_parity_weights).sum(axis=1)\n",
    "\n",
    "    # Cumulative returns calculation\n",
    "    equal_weighted_cumulative = (1 + equal_weighted_returns).cumprod()\n",
    "    risk_parity_cumulative = (1 + risk_parity_returns).cumprod()\n",
    "\n",
    "    # Drawdown calculation\n",
    "    def calculate_drawdown(returns):\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        return drawdown\n",
    "\n",
    "    equal_weighted_dd = calculate_drawdown(equal_weighted_returns)\n",
    "    risk_parity_dd = calculate_drawdown(risk_parity_returns)\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Weight comparison\n",
    "    x = np.arange(len(available_tenors))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = axes[0, 0].bar(x - width/2, equal_weights, width, label='Equal Weight',\n",
    "                           alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    bars2 = axes[0, 0].bar(x + width/2, risk_parity_weights, width, label='Risk Parity',\n",
    "                           alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "\n",
    "    axes[0, 0].set_title('Portfolio Weight Comparison')\n",
    "    axes[0, 0].set_xlabel('Tenor')\n",
    "    axes[0, 0].set_ylabel('Weight')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(\n",
    "        [f'M{int(t)}' for t in available_tenors], rotation=45)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Cumulative returns comparison\n",
    "    axes[0, 1].plot(equal_weighted_cumulative.index, equal_weighted_cumulative.values,\n",
    "                    label='Equal Weight', linewidth=2, color='blue')\n",
    "    axes[0, 1].plot(risk_parity_cumulative.index, risk_parity_cumulative.values,\n",
    "                    label='Risk Parity', linewidth=2, color='red')\n",
    "    axes[0, 1].set_title('Cumulative Returns Comparison')\n",
    "    axes[0, 1].set_ylabel('Cumulative Return')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Drawdown comparison\n",
    "    axes[1, 0].fill_between(equal_weighted_dd.index, equal_weighted_dd.values, 0,\n",
    "                            alpha=0.3, color='blue', label='Equal Weight')\n",
    "    axes[1, 0].fill_between(risk_parity_dd.index, risk_parity_dd.values, 0,\n",
    "                            alpha=0.3, color='red', label='Risk Parity')\n",
    "    axes[1, 0].set_title('Drawdown Comparison')\n",
    "    axes[1, 0].set_ylabel('Drawdown')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Risk-return scatter plot\n",
    "    individual_returns = returns_clean.mean() * 252\n",
    "    individual_vol = volatilities\n",
    "\n",
    "    axes[1, 1].scatter(individual_vol, individual_returns, s=100, alpha=0.7,\n",
    "                       c='lightgreen', edgecolor='black', label='Individual Tenors')\n",
    "\n",
    "    # Portfolio risk-return\n",
    "    equal_vol = equal_weighted_returns.std() * np.sqrt(252)\n",
    "    equal_ret = equal_weighted_returns.mean() * 252\n",
    "    rp_vol = risk_parity_returns.std() * np.sqrt(252)\n",
    "    rp_ret = risk_parity_returns.mean() * 252\n",
    "\n",
    "    axes[1, 1].scatter(equal_vol, equal_ret, s=200, color='blue',\n",
    "                       edgecolor='black', label='Equal Weight', marker='s')\n",
    "    axes[1, 1].scatter(rp_vol, rp_ret, s=200, color='red',\n",
    "                       edgecolor='black', label='Risk Parity', marker='^')\n",
    "\n",
    "    axes[1, 1].set_xlabel('Annualized Volatility')\n",
    "    axes[1, 1].set_ylabel('Annualized Return')\n",
    "    axes[1, 1].set_title('Risk-Return Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Performance statistics\n",
    "    def calculate_performance_metrics(returns):\n",
    "        annual_return = returns.mean() * 252\n",
    "        annual_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe_ratio = annual_return / annual_vol if annual_vol != 0 else 0\n",
    "        max_drawdown = calculate_drawdown(returns).min()\n",
    "        return {\n",
    "            'Annual Return': annual_return,\n",
    "            'Annual Volatility': annual_vol,\n",
    "            'Sharpe Ratio': sharpe_ratio,\n",
    "            'Max Drawdown': max_drawdown\n",
    "        }\n",
    "\n",
    "    equal_metrics = calculate_performance_metrics(equal_weighted_returns)\n",
    "    rp_metrics = calculate_performance_metrics(risk_parity_returns)\n",
    "\n",
    "    print(\"\\nPortfolio Performance Comparison:\")\n",
    "    print(\"\\nEqual Weight:\")\n",
    "    for key, value in equal_metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    print(\"\\nRisk Parity:\")\n",
    "    for key, value in rp_metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    print(\"\\nIndividual Tenor Volatilities:\")\n",
    "    for i, tenor in enumerate(available_tenors):\n",
    "        if tenor in volatilities.index:\n",
    "            print(f\"  M{tenor}: {volatilities[tenor]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Risk parity analysis requires at least 2 tenors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection and Regime Analysis\n",
    "\n",
    "### Analysis Objective - 分析目的\n",
    "- Detect abnormal market movements and structural changes (regime changes)\n",
    "- Improve risk management\n",
    "\n",
    "### Methodology - 分析手法\n",
    "- Statistical anomaly detection (Z-score, IQR method)\n",
    "- Volatility regime detection\n",
    "- Change point detection algorithms\n",
    "\n",
    "### Interpretation - グラフの見方・解釈\n",
    "- Outliers: Movements that greatly exceed normal price variations\n",
    "- Regime changes: Persistent changes in volatility or correlation structure\n",
    "- Change points: Market structure turning points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection and regime analysis\n",
    "if len(available_tenors) > 0:\n",
    "    main_tenor = available_tenors[0]\n",
    "    price_series = price_pivot[main_tenor].dropna()\n",
    "    returns = price_series.pct_change().dropna()\n",
    "\n",
    "    # 1. Statistical anomaly detection\n",
    "    # Z-score method\n",
    "    z_scores = np.abs((returns - returns.mean()) / returns.std())\n",
    "    z_threshold = 3.0\n",
    "    z_outliers = z_scores > z_threshold\n",
    "\n",
    "    # IQR method\n",
    "    Q1 = returns.quantile(0.25)\n",
    "    Q3 = returns.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = (returns < (Q1 - 1.5 * IQR)) | (returns > (Q3 + 1.5 * IQR))\n",
    "\n",
    "    # 2. Volatility regime detection\n",
    "    # 30-day rolling volatility\n",
    "    rolling_vol = returns.rolling(window=30).std() * np.sqrt(252)\n",
    "    vol_median = rolling_vol.median()\n",
    "\n",
    "    # High and low volatility periods\n",
    "    high_vol_regime = rolling_vol > vol_median * 1.5\n",
    "    low_vol_regime = rolling_vol < vol_median * 0.5\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Outlier detection\n",
    "    axes[0, 0].plot(returns.index, returns.values,\n",
    "                    linewidth=0.5, color='blue', alpha=0.7)\n",
    "    axes[0, 0].scatter(returns[z_outliers].index, returns[z_outliers].values,\n",
    "                       color='red', s=50, alpha=0.8, label=f'Z-score > {z_threshold}')\n",
    "    axes[0, 0].scatter(returns[iqr_outliers].index, returns[iqr_outliers].values,\n",
    "                       color='orange', s=30, alpha=0.8, marker='x', label='IQR Outliers')\n",
    "    axes[0, 0].set_title('Outlier Detection')\n",
    "    axes[0, 0].set_ylabel('Daily Return')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Volatility regime\n",
    "    axes[0, 1].plot(rolling_vol.index, rolling_vol.values,\n",
    "                    linewidth=1, color='blue')\n",
    "    axes[0, 1].axhline(y=vol_median, color='green',\n",
    "                       linestyle='--', alpha=0.7, label='Median')\n",
    "    axes[0, 1].axhline(y=vol_median * 1.5, color='red',\n",
    "                       linestyle='--', alpha=0.7, label='High Vol Threshold')\n",
    "    axes[0, 1].axhline(y=vol_median * 0.5, color='orange',\n",
    "                       linestyle='--', alpha=0.7, label='Low Vol Threshold')\n",
    "\n",
    "    # Regime background coloring\n",
    "    axes[0, 1].fill_between(rolling_vol.index, 0, rolling_vol.max(),\n",
    "                            where=high_vol_regime, alpha=0.2, color='red', label='High Vol Regime')\n",
    "    axes[0, 1].fill_between(rolling_vol.index, 0, rolling_vol.max(),\n",
    "                            where=low_vol_regime, alpha=0.2, color='green', label='Low Vol Regime')\n",
    "\n",
    "    axes[0, 1].set_title('Volatility Regime')\n",
    "    axes[0, 1].set_ylabel('Annualized Volatility')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Price level with outliers\n",
    "    axes[1, 0].plot(price_series.index, price_series.values,\n",
    "                    linewidth=1, color='blue')\n",
    "\n",
    "    # Mark prices on outlier dates\n",
    "    outlier_dates = returns[z_outliers | iqr_outliers].index\n",
    "    if len(outlier_dates) > 0:\n",
    "        outlier_prices = price_series.loc[outlier_dates]\n",
    "        axes[1, 0].scatter(outlier_prices.index, outlier_prices.values,\n",
    "                           color='red', s=50, alpha=0.8, label='Outlier Dates')\n",
    "\n",
    "    axes[1, 0].set_title('Price Evolution with Outliers')\n",
    "    axes[1, 0].set_ylabel('Price (USD)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Return distribution\n",
    "    normal_returns = returns[~(z_outliers | iqr_outliers)]\n",
    "    outlier_returns = returns[z_outliers | iqr_outliers]\n",
    "\n",
    "    axes[1, 1].hist(normal_returns, bins=50, alpha=0.7, color='blue',\n",
    "                    label=f'Normal (n={len(normal_returns)})', density=True)\n",
    "    if len(outlier_returns) > 0:\n",
    "        axes[1, 1].hist(outlier_returns, bins=20, alpha=0.7, color='red',\n",
    "                        label=f'Outliers (n={len(outlier_returns)})', density=True)\n",
    "\n",
    "    axes[1, 1].set_title('Return Distribution')\n",
    "    axes[1, 1].set_xlabel('Daily Return')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical summary\n",
    "    print(\"\\nOutlier Detection and Regime Analysis Results:\")\n",
    "    print(\n",
    "        f\"Z-score outliers: {z_outliers.sum()} ({z_outliers.sum()/len(returns)*100:.2f}%)\")\n",
    "    print(\n",
    "        f\"IQR outliers: {iqr_outliers.sum()} ({iqr_outliers.sum()/len(returns)*100:.2f}%)\")\n",
    "    print(\n",
    "        f\"High volatility periods: {high_vol_regime.sum()} days ({high_vol_regime.sum()/len(high_vol_regime)*100:.1f}%)\")\n",
    "    print(\n",
    "        f\"Low volatility periods: {low_vol_regime.sum()} days ({low_vol_regime.sum()/len(low_vol_regime)*100:.1f}%)\")\n",
    "    print(f\"Volatility median: {vol_median:.4f}\")\n",
    "    print(f\"Maximum volatility: {rolling_vol.max():.4f}\")\n",
    "    print(f\"Minimum volatility: {rolling_vol.min():.4f}\")\n",
    "\n",
    "    # Largest outlier date\n",
    "    if z_outliers.sum() > 0:\n",
    "        max_outlier_date = returns[z_outliers].abs().idxmax()\n",
    "        max_outlier_value = returns[max_outlier_date]\n",
    "        print(\n",
    "            f\"\\nLargest outlier: {max_outlier_date.strftime('%Y-%m-%d')} ({max_outlier_value:.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for outlier detection analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Summary - 分析結果の総括\n",
    "\n",
    "### Key Findings - 主要な発見\n",
    "1. **Spread Trading**: Inter-month price differences show mean-reverting properties, indicating statistical arbitrage opportunities\n",
    "2. **Seasonality**: Specific months or days showing bias suggest effective seasonal trading strategies\n",
    "3. **Correlation Structure**: Inter-tenor correlations change over time, limiting diversification benefits\n",
    "4. **Risk Management**: Risk parity strategies provide more stable performance than equal weighting\n",
    "5. **Anomaly Detection**: Early detection of abnormal market movements enables improved risk management\n",
    "\n",
    "### Practical Applications - 実践的な応用\n",
    "- **Portfolio Construction**: Adoption of risk parity weights\n",
    "- **Risk Management**: Early warning systems through anomaly detection\n",
    "- **Trading Strategies**: Strategies combining seasonality and spreads\n",
    "- **Dynamic Adjustment**: Portfolio rebalancing according to market regime changes\n",
    "\n",
    "### Important Notes - 注意点\n",
    "- Analysis based on historical data; does not guarantee future market performance\n",
    "- Need to consider trading costs and liquidity constraints\n",
    "- Past patterns may not continue due to market structural changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"Analysis complete. Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
